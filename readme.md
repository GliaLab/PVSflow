[![DOI](https://zenodo.org/badge/510674217.svg)](https://zenodo.org/badge/latestdoi/510674217)


# Linescan-analysis

Code for the post-processing of line-scan measurements of vessels and astrocyte endfoot diameter oscillations and simulations presented in:
* <i>Sleep cycle-dependent vascular dynamics and the predicted effects on perivascular cerebrospinal fluid flow and solute transport.</i> Laura Bojarskaite, Alexandra Vallet, Daniel M. Bjørnstad, Kristin M. Gullestad Binder, Céline Cunen, Kjell Heuser, Miroslav Kuchta, Kent-Andre Mardal, Rune Enger

## Setting up the computational environment

Instalation of our analysis pipeline is based on [docker](https://www.docker.com/). The Docker
image to be built locally by the user is defined in the [Dockerfile](https://github.com/AlexandraVallet/PVSflow/blob/master/Dockerfile). The resulting environment contains dependencies of our pipeline such as
the standard scientific Python stack (e.g. numpy, pandas) and FEniCS, gmsh. We refer to
the file for the specific version. To build the image (called in the the following **PVSflow**)
locally we navigate to the `PVSflow` repository folder and launch
```
docker build --no-cache -t PVSflow
```
Once the build process finishes we can launch the docker container with our image as```
docker run -it -v $(pwd):/home/fenics/shared PVSflow
```where we have mounted the current directory to be shared with the container. That is the `$(pwd)`
folder is accessible from within the image at `home/fenics/shared`. It is convenient if `$(pwd)`
is `PVSflow` repository folder.From inside the docker container we finally navigate to the `PVSflow` repository folder and
execute ```source setup.rc``` in order to put the analysis modules to Python path. At this point the pipeline scripts can
be launched as described below.

## Usage

### Get the data
Download and un-zip: [here](https://zenodo.org/record/7579700/files/PCS_data_analyses_and_simulations.zip) 


#### raw data (necessary for the peak to peak analysis)
Move and untar `160322_4traces.tar.gz` into `data/raw folder`

#### statistics data (necessary for the simulation preprocessing)

Move and untar `statistics.tar.gz` and then the resulting `statistics_penetrating_arterioles_WT10.tar.gz` into `data/statistics folder`.  

#### simulation results data (necessary for the dispersion analysis)

Move and unzip the `simulations.zip` into `data/simulations` folder: 

### Setup the python path

execute the command line : `source setup.rc`


### Peak to peak analysis

The main script to generate the database containing the peak to peak features is 

`/scripts/scanline-analysis.py`

It is using the module defined in 

`/src/datanalysis.py`

It will create and save databases in the folder `output/databases/`

Figures showing the details of the analysis will be saved in the folder `output/amp-analysis/`

### Database conversion for statistical analysis

The database generated by  `/scripts/scanline-analysis.py` is converted into a cvs file, used for the statistical analysis. 
Before exportation, the data is cleaned by removing : nan values, outliers, etc.

The script for conversion is `/scripts/convertcsv.py`


### Statistical analysis 

Statistical code in R with input and output files can be found [here](https://zenodo.org/record/7540400/files/Statistics_and_code.zip) 

The output files includes group/state estimates (fixed effexts) and estimates for every vessel analyzed (random effects). 

The results from this script are the statistics data, that can be downloaded and should be saved in the `data/statistics folder`.

### Preparation of simulations batch

From the statistical analysis we generate a slurm file to launch corresponding simulation of CSF flow on the super computer. These slurm files are launched through a single batch file.

The script to generate the simulation batch files is : 
`/supercomputer/batch_pythonscript.py`

The slurmfiles will be stored in the folder `output/supercomputer`


### Simulations

The script to lauch the simulation: `scripts/PVS_simulation.py`.

It must be called with several arguments that describe the PVS geometry and arteriole pulsations. The command lines can be viewed at the end of the slurm files generated in the previous stage.

For exemple a typical call for a simulations is 
```bash
python3 PVS_simulation.py -lpvs 0.02 -c0init gaussian -c0valueSAS 0 -c0valuePVS 1 -sasbc scenarioA -tend 40 -toutput 0.20053738530806994 -dt 0.0015666983227192964 -r -1 -nr 8 -nl 200 -d 1.68e-07 -s 0.0002 -xi 0.01 -j disp-d2e-07-l2e-02-baseline-card-v1e-02-id0-6-5 -ai 0.007979129036625841  -fi 9.973202736874004  -rv 0.0004984156211725926 -rpvs 0.0006981850068656416 -o '../output/simulations/'
```

You can look to the corresponding .log file in the `output/simulations/` folder to check how the simulation is running.




### Dispersion analysis

The disperison analysis is performed on the simulations output files .log and .txt. 

Those files can be directly downloaded and saved in the folder `data/simulations/`.

It uses the concentration profiles at different times, fit a gaussian profile and deduce the appatent diffusion coeficient and the associated enhancement factor. To check the output one can look to the automatically generated reports (.txt) and images in the output folder. The script generate a database with the results which is saved in the output/dispersion/ folder.

The script for the Random effect dispersion analysis is :
`/scripts/dispersion_analysis_gauss.py`



### Post processing of results and figure creations

The script to generate figures from the dispersion analysis results is :

`/scripts/simulations_plot.py` 

and the transport figures are generated with the script : 

`/scripts/transport_figures.py`
